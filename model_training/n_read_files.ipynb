{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder containing CSV files\n",
    "folder_path = 'F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: LMD-2022 [870K Elements][Labelled].csv\n",
      "Present columns: ['EventID', 'Version', 'Level', 'Task', 'Opcode', 'SystemTime', 'EventRecordID', 'Execution_ProcessID', 'ProcessID', 'ThreadID', 'Channel', 'Computer', 'UserID', 'RuleName', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'CommandLine', 'CurrentDirectory', 'ParentProcessGuid', 'ParentProcessId', 'ParentImage', 'ParentCommandLine', 'Protocol', 'SourceIsIpv6', 'SourceIp', 'SourcePort', 'DestinationIsIpv6', 'DestinationIp', 'DestinationPort', 'Signed', 'Signature', 'SignatureStatus', 'TargetFilename', 'CreationUtcTime', 'EventType', 'TargetObject', 'Details', 'QueryName', 'QueryStatus', 'IsExecutable', 'SourceProcessGuid', 'TargetProcessGuid']\n",
      "Missing columns: []\n",
      "\n",
      "File: LMD-2023 [1.75M Elements][Labelled]checked.csv\n",
      "Present columns: ['EventID', 'Version', 'Level', 'Task', 'Opcode', 'SystemTime', 'EventRecordID', 'Execution_ProcessID', 'ProcessID', 'ThreadID', 'Channel', 'Computer', 'UserID', 'RuleName', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'CommandLine', 'CurrentDirectory', 'ParentProcessGuid', 'ParentProcessId', 'ParentImage', 'ParentCommandLine', 'Protocol', 'SourceIsIpv6', 'SourceIp', 'SourcePort', 'DestinationIsIpv6', 'DestinationIp', 'DestinationPort', 'Signed', 'Signature', 'SignatureStatus', 'TargetFilename', 'CreationUtcTime', 'EventType', 'TargetObject', 'Details', 'QueryName', 'QueryStatus', 'IsExecutable', 'SourceProcessGuid', 'TargetProcessGuid']\n",
      "Missing columns: []\n",
      "\n",
      "File: LMD-2023 [1.87M Elements][Labelled]checked.csv\n",
      "Present columns: ['EventID', 'Version', 'Level', 'Task', 'Opcode', 'SystemTime', 'EventRecordID', 'Execution_ProcessID', 'ProcessID', 'ThreadID', 'Channel', 'Computer', 'UserID', 'RuleName', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'CommandLine', 'CurrentDirectory', 'ParentProcessGuid', 'ParentProcessId', 'ParentImage', 'ParentCommandLine', 'Protocol', 'SourceIsIpv6', 'SourceIp', 'SourcePort', 'DestinationIsIpv6', 'DestinationIp', 'DestinationPort', 'Signed', 'Signature', 'SignatureStatus', 'TargetFilename', 'CreationUtcTime', 'EventType', 'TargetObject', 'Details', 'QueryName', 'QueryStatus', 'IsExecutable', 'SourceProcessGuid', 'TargetProcessGuid']\n",
      "Missing columns: []\n",
      "\n",
      "File: LMD-2023 [2.3M Elements][Labelled]checked.csv\n",
      "Present columns: ['EventID', 'Version', 'Level', 'Task', 'Opcode', 'SystemTime', 'EventRecordID', 'Execution_ProcessID', 'ProcessID', 'ThreadID', 'Channel', 'Computer', 'UserID', 'RuleName', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'CommandLine', 'CurrentDirectory', 'ParentProcessGuid', 'ParentProcessId', 'ParentImage', 'ParentCommandLine', 'Protocol', 'SourceIsIpv6', 'SourceIp', 'SourcePort', 'DestinationIsIpv6', 'DestinationIp', 'DestinationPort', 'Signed', 'Signature', 'SignatureStatus', 'TargetFilename', 'CreationUtcTime', 'EventType', 'TargetObject', 'Details', 'QueryName', 'QueryStatus', 'IsExecutable', 'SourceProcessGuid', 'TargetProcessGuid']\n",
      "Missing columns: []\n"
     ]
    }
   ],
   "source": [
    "# List of features (columns) to check\n",
    "selected_features = [\n",
    "    \"EventID\", \"Version\", \"Level\", \"Task\", \"Opcode\", \"SystemTime\", \n",
    "    \"EventRecordID\", \"Execution_ProcessID\", \"ProcessID\", \"ThreadID\", \"Channel\", \n",
    "    \"Computer\", \"UserID\", \"RuleName\", \"UtcTime\", \"ProcessGuid\", \"ProcessId\", \n",
    "    \"Image\", \"CommandLine\", \"CurrentDirectory\", \"ParentProcessGuid\", \n",
    "    \"ParentProcessId\", \"ParentImage\", \"ParentCommandLine\", \"Protocol\", \n",
    "    \"SourceIsIpv6\", \"SourceIp\", \"SourcePort\", \"DestinationIsIpv6\", \"DestinationIp\", \n",
    "    \"DestinationPort\", \"Signed\", \"Signature\", \"SignatureStatus\", \"TargetFilename\", \n",
    "    \"CreationUtcTime\", \"EventType\", \"TargetObject\", \"Details\", \"QueryName\", \n",
    "    \"QueryStatus\", \"IsExecutable\", \"SourceProcessGuid\", \"TargetProcessGuid\"\n",
    "]\n",
    "\n",
    "# Function to check columns in each CSV file\n",
    "def check_columns_in_files(folder_path):\n",
    "    # Iterate through all CSV files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Check if the file is a CSV file\n",
    "        if file_path.endswith('.csv'):\n",
    "            # Read only the header (first row) to get available columns\n",
    "            available_columns = pd.read_csv(file_path, nrows=1).columns\n",
    "            \n",
    "            # Determine which columns are present and which are missing\n",
    "            present_columns = [col for col in selected_features if col in available_columns]\n",
    "            missing_columns = [col for col in selected_features if col not in available_columns]\n",
    "            \n",
    "            # Print the results for each file\n",
    "            print(f\"\\nFile: {file_name}\")\n",
    "            print(f\"Present columns: {present_columns}\")\n",
    "            print(f\"Missing columns: {missing_columns}\")\n",
    "\n",
    "# Run the column check\n",
    "check_columns_in_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "def preprocess_chunk(data):\n",
    "    \"\"\"\n",
    "    Preprocess a single chunk of data according to the specified encoding and transformations.\n",
    "    Parameters:\n",
    "        data (DataFrame): A chunk of data to preprocess.\n",
    "    Returns:\n",
    "        DataFrame: The preprocessed chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define feature categories based on requirements\n",
    "\n",
    "    # Low Cardinality Features (Categorical features with few unique values)\n",
    "    low_cardinality_features = [\n",
    "        'Level',            # log.level\n",
    "        'Task',             # winlog.task\n",
    "        'Opcode',           # winlog.opcode\n",
    "        'Channel',          # winlog.channel\n",
    "        'Signed',           # winlog.event_data.Signed\n",
    "        'SignatureStatus'   # winlog.event_data.SignatureStatus\n",
    "    ]\n",
    "\n",
    "    # High Cardinality Features (Categorical features with many unique values)\n",
    "    high_cardinality_features = [\n",
    "        'Computer',         # winlog.computer_name\n",
    "        'UserID',           # winlog.user.identifier\n",
    "        'RuleName',         # winlog.event_data.RuleName\n",
    "        'ProcessGuid',      # winlog.event_data.ProcessGuid\n",
    "        'ParentProcessGuid',# winlog.event_data.ParentProcessGuid\n",
    "        'ParentImage'       # winlog.event_data.ParentImage\n",
    "    ]\n",
    "\n",
    "    # Textual Features (Text data requiring tokenization, including paths)\n",
    "    textual_features = [\n",
    "        'message',          # Message field if available, not listed here but common\n",
    "        'Image',            # winlog.event_data.Image\n",
    "        'TargetFilename',   # winlog.event_data.TargetFilename\n",
    "        'CommandLine',      # winlog.event_data.CommandLine\n",
    "        'CurrentDirectory', # winlog.event_data.CurrentDirectory\n",
    "        'ParentImage',      # winlog.event_data.ParentImage\n",
    "        'ParentCommandLine' # winlog.event_data.ParentCommandLine\n",
    "    ]\n",
    "\n",
    "    # ID Features (Numerical IDs that can benefit from binary encoding)\n",
    "    id_features = [\n",
    "        'EventRecordID',        # winlog.record_id\n",
    "        'Execution_ProcessID',  # winlog.process.pid\n",
    "        'ProcessID',            # winlog.event_data.ProcessId\n",
    "        'ThreadID',             # winlog.process.thread.id\n",
    "        'SourceProcessGuid',    # winlog.event_data.SourceProcessGuid\n",
    "        'TargetProcessGuid'     # winlog.event_data.TargetProcessGuid\n",
    "    ]\n",
    "\n",
    "    # Binary Features (True/False or binary values)\n",
    "    binary_features = [\n",
    "        'SourceIsIpv6',         # winlog.event_data.SourceIsIpv6\n",
    "        'DestinationIsIpv6',    # winlog.event_data.DestinationIsIpv6\n",
    "        'IsExecutable'          # winlog.event_data.IsExecutable\n",
    "    ]\n",
    "\n",
    "    # Network Features (IPs and ports that can be split into octets or encoded)\n",
    "    network_features = [\n",
    "        'SourceIp',             # winlog.event_data.SourceIp\n",
    "        'DestinationIp',        # winlog.event_data.DestinationIp\n",
    "        'SourcePort',           # winlog.event_data.SourcePort\n",
    "        'DestinationPort'       # winlog.event_data.DestinationPort\n",
    "    ]\n",
    "\n",
    "    # Miscellaneous Features (Features that do not fit into other categories)\n",
    "     \n",
    "\n",
    "    # Helper function for binary encoding (converting to binary and one-hot encoding)\n",
    "    def binary_encode(value, length=8):\n",
    "        # Convert to binary, fill up to desired length, then split each bit\n",
    "        return [int(bit) for bit in f\"{value:0{length}b}\"]\n",
    "\n",
    "    # Preprocess low cardinality features with Label Encoding + Binary Encoding\n",
    "    for feature in low_cardinality_features:\n",
    "        le = LabelEncoder()\n",
    "        data[feature] = le.fit_transform(data[feature])\n",
    "        # Apply binary encoding (binary representation of integer label)\n",
    "        binary_encoded_df = data[feature].apply(lambda x: binary_encode(x, length=8))\n",
    "        binary_encoded_df = pd.DataFrame(binary_encoded_df.tolist(), index=data.index)\n",
    "        data = data.drop(columns=[feature]).join(binary_encoded_df, rsuffix=f\"_{feature}_bin\")\n",
    "\n",
    "    # Preprocess high cardinality features with Hashing + Binary Encoding\n",
    "    for feature in high_cardinality_features:\n",
    "        hasher = HashingVectorizer(n_features=8, binary=True)  # Adjust n_features based on dataset size\n",
    "        hashed_data = hasher.transform(data[feature].astype(str)).toarray()\n",
    "        hashed_df = pd.DataFrame(hashed_data, columns=[f\"{feature}_hash_{i}\" for i in range(hashed_data.shape[1])])\n",
    "        data = pd.concat([data, hashed_df], axis=1).drop(columns=[feature])\n",
    "\n",
    "    # Tokenize textual data with slashes for paths and prepare for positional encoding\n",
    "    for feature in textual_features:\n",
    "        data[feature] = data[feature].str.split('/')  # Tokenize by slashes for paths\n",
    "        data[feature] = data[feature].apply(lambda x: [token for token in x if token])\n",
    "\n",
    "    # Binary encode numerical features that are treated as IDs\n",
    "    for feature in id_features:\n",
    "        data[feature] = data[feature].apply(lambda x: binary_encode(int(x), length=16))\n",
    "\n",
    "    # Process network features by splitting IPs into octets and binary encoding each octet\n",
    "    for feature in network_features:\n",
    "        data[feature] = data[feature].apply(lambda ip: ip.split('.') if isinstance(ip, str) else [])\n",
    "        # Convert octets to binary encoded form\n",
    "        octet_df = data[feature].apply(lambda x: [binary_encode(int(octet), length=8) for octet in x])\n",
    "        flattened_octet_df = pd.DataFrame(octet_df.tolist(), index=data.index)\n",
    "        data = pd.concat([data, flattened_octet_df], axis=1).drop(columns=[feature])\n",
    "\n",
    "    # Convert binary features to 1 and 0\n",
    "    for feature in binary_features:\n",
    "        data[feature] = data[feature].astype(int)\n",
    "\n",
    "    return data  # Return the preprocessed chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential'\n",
    "output_folder = 'F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_preprocessed'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def load_and_process_in_chunks(input_folder, output_folder, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Load and preprocess data in chunks from CSV files, applying transformations, \n",
    "    and save the processed chunks to the output folder.\n",
    "    \"\"\"\n",
    "    # Get a list of CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    # Process each file in the input folder\n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        # Initialize chunk processing\n",
    "        chunk_number = 0\n",
    "        \n",
    "        # Load and process each chunk\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            # Apply preprocessing to the chunk\n",
    "            processed_chunk = preprocess_chunk(chunk)\n",
    "            \n",
    "            # Save processed chunk to output folder\n",
    "            save_processed_chunk(processed_chunk, output_folder, file_name, chunk_number)\n",
    "            \n",
    "            print(f\"Processed and saved chunk {chunk_number} for file {file_name}\")\n",
    "            chunk_number += 1\n",
    "\n",
    "\n",
    "def save_processed_chunk(processed_chunk, output_folder, original_file_name, chunk_number):\n",
    "    \"\"\"\n",
    "    Save the processed chunk to the output folder with a chunk identifier in the file name.\n",
    "    \"\"\"\n",
    "    # Construct a unique file name for each chunk\n",
    "    base_name = os.path.splitext(original_file_name)[0]\n",
    "    output_file_name = f\"{base_name}_processed_chunk_{chunk_number}.csv\"\n",
    "    output_path = os.path.join(output_folder, output_file_name)\n",
    "    \n",
    "    # Save the processed chunk as a CSV\n",
    "    processed_chunk.to_csv(output_path, index=False)\n",
    "    print(f\"Saved processed chunk to {output_path}\")\n",
    "\n",
    "# Execute the function to process and save all files in chunks\n",
    "load_and_process_in_chunks(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File 1/4: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2022 [870K Elements][Labelled].csv\n",
      "Processing File 2/4: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [1.75M Elements][Labelled]checked.csv\n",
      "Processing File 3/4: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [1.87M Elements][Labelled]checked.csv\n",
      "Processing File 4/4: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [2.3M Elements][Labelled]checked.csv\n",
      "\n",
      "File: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2022 [870K Elements][Labelled].csv\n",
      "Columns: ['SystemTime', 'UtcTime', 'Image', 'Description', 'Product', 'Company', 'CommandLine', 'CurrentDirectory', 'ParentImage', 'ParentCommandLine', 'Initiated', 'SourceIsIpv6', 'DestinationIsIpv6', 'State', 'ImageLoaded', 'Signed', 'SignatureStatus', 'SourceProcessId', 'SourceThreadId', 'SourceImage', 'TargetImage', 'CallTrace', 'SourceUser', 'TargetUser', 'TargetFilename', 'CreationUtcTime', 'TargetObject', 'Details', 'PipeName', 'QueryStatus', 'IsExecutable', 'PreviousCreationUtcTime', 'Label', 'Level_hash', 'Task_hash', 'Protocol_hash', 'Version2_hash', 'SchemaVersion_hash', 'Signature_hash', 'EventType_hash', 'StartFunction_hash', 'ID_hash', 'Configuration_hash', 'ConfigurationFileHash_hash', 'IntegrityLevel_hash', 'UserID_hash', 'Computer_hash', 'RuleName_hash', 'TerminalSessionId_hash', 'Version_hash', 'User_hash', 'ParentUser_hash', 'FileVersion_hash', 'Guid_hash', 'ProcessGuid_hash', 'ParentProcessGuid_hash', 'SourceProcessGUID_hash', 'TargetProcessGUID_hash', 'SourceProcessGuid_hash', 'LogonGuid_hash', 'ThreadID_hash', 'LogonId_hash', 'SourceHostname_hash', 'OriginalFileName_hash', 'TargetProcessGuid_hash', 'DestinationHostname_hash', 'SourcePortName_hash', 'DestinationPortName_hash', 'StartAddress_hash', 'StartModule_hash', 'NewThreadId_hash', 'GrantedAccess_hash', 'QueryName_hash', 'QueryResults_hash', 'Hashes_hash', 'Hash_hash', 'Contents_hash', 'SourceIp_hash', 'SourceIp_prefix_hash', 'DestinationIp_hash', 'DestinationIp_prefix_hash', 'EventID_hash', 'ProcessID_hash', 'Execution_ProcessID_hash', 'ParentProcessId_hash', 'TargetProcessId_hash', 'EventRecordID_hash', 'SourcePort_hash', 'DestinationPort_hash', 'ProcessId_hash']\n",
      "\n",
      "File: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [1.75M Elements][Labelled]checked.csv\n",
      "Columns: ['SystemTime', 'Label', 'UtcTime', 'Image', 'Description', 'Product', 'Company', 'CommandLine', 'CurrentDirectory', 'ParentImage', 'ParentCommandLine', 'Initiated', 'SourceIsIpv6', 'DestinationIsIpv6', 'State', 'ImageLoaded', 'Signed', 'SignatureStatus', 'SourceProcessId', 'SourceThreadId', 'SourceImage', 'TargetImage', 'CallTrace', 'SourceUser', 'TargetUser', 'TargetFilename', 'CreationUtcTime', 'TargetObject', 'Details', 'PipeName', 'QueryStatus', 'IsExecutable', 'PreviousCreationUtcTime', 'Level_hash', 'Task_hash', 'Protocol_hash', 'Version2_hash', 'SchemaVersion_hash', 'Signature_hash', 'EventType_hash', 'StartFunction_hash', 'ID_hash', 'Configuration_hash', 'ConfigurationFileHash_hash', 'IntegrityLevel_hash', 'UserID_hash', 'Computer_hash', 'RuleName_hash', 'TerminalSessionId_hash', 'Version_hash', 'User_hash', 'ParentUser_hash', 'FileVersion_hash', 'Guid_hash', 'ProcessGuid_hash', 'ParentProcessGuid_hash', 'SourceProcessGUID_hash', 'TargetProcessGUID_hash', 'SourceProcessGuid_hash', 'LogonGuid_hash', 'ThreadID_hash', 'LogonId_hash', 'SourceHostname_hash', 'OriginalFileName_hash', 'TargetProcessGuid_hash', 'DestinationHostname_hash', 'SourcePortName_hash', 'DestinationPortName_hash', 'StartAddress_hash', 'StartModule_hash', 'NewThreadId_hash', 'GrantedAccess_hash', 'QueryName_hash', 'QueryResults_hash', 'Hashes_hash', 'Hash_hash', 'Contents_hash', 'SourceIp_hash', 'SourceIp_prefix_hash', 'DestinationIp_hash', 'DestinationIp_prefix_hash', 'EventID_hash', 'ProcessID_hash', 'Execution_ProcessID_hash', 'ParentProcessId_hash', 'TargetProcessId_hash', 'EventRecordID_hash', 'SourcePort_hash', 'DestinationPort_hash', 'ProcessId_hash']\n",
      "\n",
      "File: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [1.87M Elements][Labelled]checked.csv\n",
      "Columns: ['Unnamed: 0', 'UtcTime', 'Image', 'Description', 'Product', 'Company', 'CommandLine', 'CurrentDirectory', 'ParentImage', 'ParentCommandLine', 'Initiated', 'SourceIsIpv6', 'DestinationIsIpv6', 'State', 'ImageLoaded', 'Signed', 'SignatureStatus', 'SourceProcessId', 'SourceThreadId', 'SourceImage', 'TargetImage', 'CallTrace', 'SourceUser', 'TargetUser', 'TargetFilename', 'CreationUtcTime', 'TargetObject', 'Details', 'PipeName', 'QueryStatus', 'IsExecutable', 'PreviousCreationUtcTime', 'Label', 'SystemTime', 'Level_hash', 'Task_hash', 'Protocol_hash', 'Version2_hash', 'SchemaVersion_hash', 'Signature_hash', 'EventType_hash', 'StartFunction_hash', 'ID_hash', 'Configuration_hash', 'ConfigurationFileHash_hash', 'IntegrityLevel_hash', 'UserID_hash', 'Computer_hash', 'RuleName_hash', 'TerminalSessionId_hash', 'Version_hash', 'User_hash', 'ParentUser_hash', 'FileVersion_hash', 'Guid_hash', 'ProcessGuid_hash', 'ParentProcessGuid_hash', 'SourceProcessGUID_hash', 'TargetProcessGUID_hash', 'SourceProcessGuid_hash', 'LogonGuid_hash', 'ThreadID_hash', 'LogonId_hash', 'SourceHostname_hash', 'OriginalFileName_hash', 'TargetProcessGuid_hash', 'DestinationHostname_hash', 'SourcePortName_hash', 'DestinationPortName_hash', 'StartAddress_hash', 'StartModule_hash', 'NewThreadId_hash', 'GrantedAccess_hash', 'QueryName_hash', 'QueryResults_hash', 'Hashes_hash', 'Hash_hash', 'Contents_hash', 'SourceIp_hash', 'SourceIp_prefix_hash', 'DestinationIp_hash', 'DestinationIp_prefix_hash', 'EventID_hash', 'ProcessID_hash', 'Execution_ProcessID_hash', 'ParentProcessId_hash', 'TargetProcessId_hash', 'EventRecordID_hash', 'SourcePort_hash', 'DestinationPort_hash', 'ProcessId_hash']\n",
      "\n",
      "File: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [2.3M Elements][Labelled]checked.csv\n",
      "Columns: ['UtcTime', 'Image', 'Description', 'Product', 'Company', 'CommandLine', 'CurrentDirectory', 'ParentImage', 'ParentCommandLine', 'Initiated', 'SourceIsIpv6', 'DestinationIsIpv6', 'State', 'ImageLoaded', 'Signed', 'SignatureStatus', 'SourceProcessId', 'SourceThreadId', 'SourceImage', 'TargetImage', 'CallTrace', 'SourceUser', 'TargetUser', 'TargetFilename', 'CreationUtcTime', 'TargetObject', 'Details', 'PipeName', 'QueryStatus', 'IsExecutable', 'PreviousCreationUtcTime', 'Label', 'SystemTime', 'Level_hash', 'Task_hash', 'Protocol_hash', 'Version2_hash', 'SchemaVersion_hash', 'Signature_hash', 'EventType_hash', 'StartFunction_hash', 'ID_hash', 'Configuration_hash', 'ConfigurationFileHash_hash', 'IntegrityLevel_hash', 'UserID_hash', 'Computer_hash', 'RuleName_hash', 'TerminalSessionId_hash', 'Version_hash', 'User_hash', 'ParentUser_hash', 'FileVersion_hash', 'Guid_hash', 'ProcessGuid_hash', 'ParentProcessGuid_hash', 'SourceProcessGUID_hash', 'TargetProcessGUID_hash', 'SourceProcessGuid_hash', 'LogonGuid_hash', 'ThreadID_hash', 'LogonId_hash', 'SourceHostname_hash', 'OriginalFileName_hash', 'TargetProcessGuid_hash', 'DestinationHostname_hash', 'SourcePortName_hash', 'DestinationPortName_hash', 'StartAddress_hash', 'StartModule_hash', 'NewThreadId_hash', 'GrantedAccess_hash', 'QueryName_hash', 'QueryResults_hash', 'Hashes_hash', 'Hash_hash', 'Contents_hash', 'SourceIp_hash', 'SourceIp_prefix_hash', 'DestinationIp_hash', 'DestinationIp_prefix_hash', 'EventID_hash', 'ProcessID_hash', 'Execution_ProcessID_hash', 'ParentProcessId_hash', 'TargetProcessId_hash', 'EventRecordID_hash', 'SourcePort_hash', 'DestinationPort_hash', 'ProcessId_hash']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = \"F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\"\n",
    "\n",
    "# List to store column names for each file\n",
    "file_column_names = {}\n",
    "\n",
    "# Read all CSV files in the folder\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Process each CSV file\n",
    "for file_index, file_path in enumerate(csv_files, start=1):\n",
    "    print(f\"Processing File {file_index}/{len(csv_files)}: {file_path}\")\n",
    "    try:\n",
    "        # Read only the header row\n",
    "        column_names = pd.read_csv(file_path, nrows=0).columns.tolist()\n",
    "        file_column_names[file_path] = column_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Display results\n",
    "for file, columns in file_column_names.items():\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"Columns: {columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Is GPU available:  False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Is GPU available: \", tf.test.is_gpu_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
