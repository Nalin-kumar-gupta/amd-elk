{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU threads are limited to 2.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Add, Multiply, LayerNormalization, Dropout\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"2\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"2\"\n",
    "\n",
    "print(\"CPU threads are limited to 2.\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000  # Size of vocabulary for hash and path features\n",
    "embedding_dim = 16  # Embedding dimension\n",
    "max_path_length = 20  # Maximum length for path sequences\n",
    "\n",
    "save_model_dir = \"F:/models\"\n",
    "best_model_path = f\"{save_model_dir}/best_model.keras\"\n",
    "epoch_model_path = f\"{save_model_dir}/model_epoch_{'{epoch:02d}'}.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_embedding_inputs(hash_features):\n",
    "    \"\"\"\n",
    "    Create embedding layers and input layers for hash-based features.\n",
    "    \"\"\"\n",
    "    hash_inputs = []\n",
    "    hash_embeddings = []\n",
    "\n",
    "    for feature in hash_features:\n",
    "        input_layer = Input(shape=(1,), name=f\"{feature}_input\")\n",
    "        embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1)(input_layer)\n",
    "        hash_inputs.append(input_layer)\n",
    "        hash_embeddings.append(Flatten()(embedding_layer))  # Flatten to use with Dense layers later\n",
    "\n",
    "    return hash_inputs, hash_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_embedding_inputs(path_features, use_shared_embedding=True):\n",
    "    \"\"\"\n",
    "    Create embedding layers and input layers for path-based features.\n",
    "    If `use_shared_embedding` is True, a single embedding layer is shared for all path features.\n",
    "    \"\"\"\n",
    "    path_inputs = []\n",
    "    path_embeddings = []\n",
    "\n",
    "    if use_shared_embedding:\n",
    "        shared_embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_path_length)\n",
    "\n",
    "        for feature in path_features:\n",
    "            input_layer = Input(shape=(max_path_length,), name=f\"{feature}_input\")\n",
    "            embedding_output = shared_embedding_layer(input_layer)\n",
    "            path_inputs.append(input_layer)\n",
    "            path_embeddings.append(Flatten()(embedding_output))  # Flatten to combine with other features\n",
    "\n",
    "    else:\n",
    "        for feature in path_features:\n",
    "            input_layer = Input(shape=(max_path_length,), name=f\"{feature}_input\")\n",
    "            embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_path_length)(\n",
    "                input_layer\n",
    "            )\n",
    "            path_inputs.append(input_layer)\n",
    "            path_embeddings.append(Flatten()(embedding_layer))\n",
    "\n",
    "    return path_inputs, path_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_textual_embedding_inputs(textual_features, tokenizer_vocab_size):\n",
    "    \"\"\"\n",
    "    Create embedding layers and input layers for textual features.\n",
    "    Each feature will have its own embedding layer.\n",
    "    \"\"\"\n",
    "    text_inputs = []\n",
    "    text_embeddings = []\n",
    "\n",
    "    for feature in textual_features:\n",
    "        input_layer = Input(shape=(max_path_length,), name=f\"{feature}_input\")  # Textual features are tokenized\n",
    "        embedding_layer = Embedding(input_dim=tokenizer_vocab_size, output_dim=embedding_dim, input_length=max_path_length)(\n",
    "            input_layer\n",
    "        )\n",
    "        text_inputs.append(input_layer)\n",
    "        text_embeddings.append(Flatten()(embedding_layer))  # Flatten for further processing\n",
    "\n",
    "    return text_inputs, text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numerical_inputs(numerical_features):\n",
    "    \"\"\"\n",
    "    Create input layers for numerical (normalized) features.\n",
    "    \"\"\"\n",
    "    numerical_inputs = []\n",
    "    for feature in numerical_features:\n",
    "        input_layer = Input(shape=(1,), name=f\"{feature}_input\")  # Each feature has its own input layer\n",
    "        numerical_inputs.append(input_layer)\n",
    "\n",
    "    return numerical_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_features = [\n",
    "    \"Level_hash\", \"Task_hash\", \"Protocol_hash\", \"Version2_hash\", \"SchemaVersion_hash\",\n",
    "    \"Signature_hash\", \"EventType_hash\", \"StartFunction_hash\", \"ID_hash\",\n",
    "    \"Configuration_hash\", \"ConfigurationFileHash_hash\", \"IntegrityLevel_hash\",\n",
    "    \"UserID_hash\", \"Computer_hash\", \"RuleName_hash\", \"TerminalSessionId_hash\",\n",
    "    \"Version_hash\", \"User_hash\", \"ParentUser_hash\", \"FileVersion_hash\",\n",
    "    \"Guid_hash\", \"ProcessGuid_hash\", \"ParentProcessGuid_hash\", \"SourceProcessGUID_hash\",\n",
    "    \"TargetProcessGUID_hash\", \"SourceProcessGuid_hash\", \"LogonGuid_hash\",\n",
    "    \"ThreadID_hash\", \"LogonId_hash\", \"SourceHostname_hash\", \"OriginalFileName_hash\",\n",
    "    \"TargetProcessGuid_hash\", \"DestinationHostname_hash\", \"SourcePortName_hash\",\n",
    "    \"DestinationPortName_hash\", \"StartAddress_hash\", \"StartModule_hash\", \"NewThreadId_hash\",\n",
    "    \"GrantedAccess_hash\", \"QueryName_hash\", \"QueryResults_hash\", \"Hashes_hash\", \n",
    "    \"Hash_hash\", \"Contents_hash\", \"SourceIp_hash\", \"SourceIp_prefix_hash\", \n",
    "    \"DestinationIp_hash\", \"DestinationIp_prefix_hash\", \"EventID_hash\", \n",
    "    \"ProcessID_hash\", \"Execution_ProcessID_hash\", \"ParentProcessId_hash\", \n",
    "    \"TargetProcessId_hash\", \"EventRecordID_hash\", \"SourcePort_hash\", \n",
    "    \"DestinationPort_hash\", \"ProcessId_hash\"\n",
    "]\n",
    " # Replace with your hash features\n",
    "path_features = [\n",
    "    \"Image\", \"ParentImage\", \"SourceImage\", \"TargetImage\", \"ImageLoaded\",\n",
    "    \"CurrentDirectory\", \"ParentCommandLine\", \"CommandLine\", \"CallTrace\", \n",
    "    \"TargetFilename\", \"TargetObject\", \"Details\", \"PipeName\", \"QueryName\"\n",
    "]\n",
    "\n",
    "  # Replace with your path features\n",
    "textual_features = [\n",
    "    \"Description\", \"Product\", \"Company\"\n",
    "]\n",
    " # Replace with your textual features\n",
    "numerical_features = [\n",
    "    \"SystemTime\", \"UtcTime\", \"CreationUtcTime\", \"PreviousCreationUtcTime\"\n",
    "]\n",
    "# Create embedding and input layers\n",
    "hash_inputs, hash_embeddings = create_hash_embedding_inputs(hash_features)\n",
    "path_inputs, path_embeddings = create_path_embedding_inputs(path_features, use_shared_embedding=True)\n",
    "text_inputs, text_embeddings = create_textual_embedding_inputs(textual_features, tokenizer_vocab_size=10000)\n",
    "numerical_inputs = create_numerical_inputs(numerical_features)\n",
    "\n",
    "# Combine all embeddings and inputs\n",
    "all_inputs = hash_inputs + path_inputs + text_inputs + numerical_inputs\n",
    "all_embeddings = hash_embeddings + path_embeddings + text_embeddings\n",
    "\n",
    "# Numerical features are already tensors; no embedding layer required\n",
    "numerical_tensors = numerical_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gating_mechanism(x, units, dense_layer):\n",
    "    \"\"\"Applies a gating mechanism to input x using a pre-defined Dense layer.\"\"\"\n",
    "    activation_layer = dense_layer(x)\n",
    "    return Multiply()([x, activation_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"Calculates scaled dot-product attention.\"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, query, key, value, mask=None):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        query = self.split_heads(self.wq(query), batch_size)\n",
    "        key = self.split_heads(self.wk(key), batch_size)\n",
    "        value = self.split_heads(self.wv(value), batch_size)\n",
    "        \n",
    "        attention, weights = scaled_dot_product_attention(query, key, value, mask)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.num_heads * self.depth))\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalFusionTransformer(tf.keras.Model):\n",
    "    def __init__(self, static_input_dim, time_varying_input_dim, output_dim, d_model, num_heads, dropout_rate=0.1):\n",
    "        super(TemporalFusionTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Static embeddings\n",
    "        self.static_embedding = Dense(d_model)\n",
    "        self.gating_dense = Dense(d_model, activation=\"sigmoid\")  # Define Dense layer for gating\n",
    "\n",
    "        # Temporal embeddings\n",
    "        self.temporal_embedding = Dense(d_model)\n",
    "\n",
    "        # Multi-head attention layers\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # Feed-forward layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(d_model, activation=\"relu\"),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        # Normalization and dropout\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "        # Pooling layer to aggregate time steps\n",
    "        self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = Dense(output_dim, activation=\"sigmoid\")  # Binary classification\n",
    "\n",
    "    def call(self, static_inputs, temporal_inputs):\n",
    "        # Static processing\n",
    "        static_embedded = self.static_embedding(static_inputs)\n",
    "\n",
    "        # Apply gating mechanism\n",
    "        gated_static = gating_mechanism(static_embedded, self.d_model, self.gating_dense)\n",
    "\n",
    "        # Temporal processing\n",
    "        temporal_embedded = self.temporal_embedding(temporal_inputs)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_output, _ = self.multi_head_attention(\n",
    "            query=temporal_embedded, key=temporal_embedded, value=temporal_embedded\n",
    "        )\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output = self.norm1(attention_output + temporal_embedded)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(attention_output)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        ffn_output = self.norm2(ffn_output + attention_output)\n",
    "\n",
    "        # Combine static and temporal\n",
    "        gated_static_expanded = tf.expand_dims(gated_static, axis=1)\n",
    "        combined = Add()([ffn_output, gated_static_expanded])\n",
    "\n",
    "        # Aggregate across timesteps (pooling)\n",
    "        pooled_output = self.pooling(combined)\n",
    "\n",
    "        # Output\n",
    "        output = self.output_layer(pooled_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ static_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">217</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_fusion_tr… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">43,713</span> │ static_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TemporalFusionTra…</span> │                   │            │ temporal_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ static_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m217\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_fusion_tr… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │     \u001b[38;5;34m43,713\u001b[0m │ static_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mTemporalFusionTra…\u001b[0m │                   │            │ temporal_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,713</span> (170.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,713\u001b[0m (170.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,713</span> (170.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,713\u001b[0m (170.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Define TFT with Preprocessed Features\n",
    "def build_tft_with_embeddings(\n",
    "    d_model, num_heads, dropout_rate, static_input_dim, temporal_input_dim\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a Temporal Fusion Transformer (TFT) model.\n",
    "\n",
    "    Args:\n",
    "        d_model: Dimensionality of the model layers.\n",
    "        num_heads: Number of attention heads in the transformer.\n",
    "        dropout_rate: Dropout rate for regularization.\n",
    "        static_input_dim: Dimensionality of static inputs.\n",
    "        temporal_input_dim: Dimensionality of temporal inputs.\n",
    "\n",
    "    Returns:\n",
    "        A compiled TFT model.\n",
    "    \"\"\"\n",
    "    # Input layers for static and temporal features\n",
    "    static_input_layer = Input(shape=(static_input_dim,), name=\"static_input\")\n",
    "    temporal_input_layer = Input(shape=(None, temporal_input_dim), name=\"temporal_input\")\n",
    "\n",
    "    # Instantiate the Temporal Fusion Transformer model\n",
    "    tft = TemporalFusionTransformer(\n",
    "        static_input_dim=static_input_dim,\n",
    "        time_varying_input_dim=temporal_input_dim,\n",
    "        output_dim=1,  # Binary classification\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout_rate=dropout_rate,\n",
    "    )\n",
    "\n",
    "    # Get outputs from the TFT model\n",
    "    outputs = tft(static_input_layer, temporal_input_layer)\n",
    "\n",
    "    # Create and compile the Keras model\n",
    "    model = Model(inputs=[static_input_layer, temporal_input_layer], outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Step 2: Prepare Inputs for Model\n",
    "def prepare_tft_inputs(hash_embeddings, path_embeddings, text_embeddings, numerical_inputs):\n",
    "    \"\"\"\n",
    "    Prepares inputs for the TFT model.\n",
    "\n",
    "    Args:\n",
    "        hash_embeddings: List of tensors from hash feature embeddings.\n",
    "        path_embeddings: List of tensors from path feature embeddings.\n",
    "        text_embeddings: List of tensors from textual feature embeddings.\n",
    "        numerical_inputs: List of tensors for normalized numerical features.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing static and temporal input tensors.\n",
    "    \"\"\"\n",
    "    # Concatenate static embeddings (hash, path, and text features)\n",
    "    static_embeddings = Concatenate()(hash_embeddings + path_embeddings + text_embeddings)\n",
    "\n",
    "    # Concatenate numerical inputs for temporal features\n",
    "    temporal_inputs = Concatenate()(numerical_inputs)\n",
    "    \n",
    "    return static_embeddings, temporal_inputs\n",
    "\n",
    "\n",
    "# Combine embeddings for static and temporal features\n",
    "static_inputs, temporal_inputs = prepare_tft_inputs(hash_embeddings, path_embeddings, text_embeddings, numerical_inputs)\n",
    "\n",
    "# Build the TFT model\n",
    "tft_model = build_tft_with_embeddings(\n",
    "    d_model=64,\n",
    "    num_heads=8,\n",
    "    dropout_rate=0.1,\n",
    "    static_input_dim=217,  # Replace with the actual shape from the generator\n",
    "    temporal_input_dim=4,  # Replace with actual temporal input dimensions\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: View Model Summary\n",
    "tft_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_from_folder(\n",
    "    folder_path, chunk_size, hash_features, path_features, textual_features, numerical_features, tokenizer, vocab_size, split_ratio=0.8, mode=\"train\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generator to produce static and temporal inputs along with labels, split dynamically for train/validation,\n",
    "    with memory optimizations.\n",
    "    \"\"\"\n",
    "    csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):  # Allow pandas to infer types\n",
    "            # Shuffle the chunk\n",
    "            chunk = chunk.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            # Split into training and validation\n",
    "            split_index = int(len(chunk) * split_ratio)\n",
    "            if mode == \"train\":\n",
    "                data_chunk = chunk.iloc[:split_index]\n",
    "            else:  # mode == \"validation\"\n",
    "                data_chunk = chunk.iloc[split_index:]\n",
    "\n",
    "            # Preprocess static inputs (hash, path, and textual features)\n",
    "            static_inputs = []\n",
    "            for feature in hash_features:\n",
    "                if feature in data_chunk.columns:\n",
    "                    static_inputs.append(data_chunk[feature].fillna(0).astype(\"int32\").values)\n",
    "            for feature in path_features:\n",
    "                if feature in data_chunk.columns:\n",
    "                    tokenized = tokenizer.texts_to_sequences(data_chunk[feature].fillna(\"\").astype(str))\n",
    "                    padded = pad_sequences(tokenized, maxlen=10, padding=\"post\", truncating=\"post\")\n",
    "                    static_inputs.append(padded)\n",
    "            for feature in textual_features:\n",
    "                if feature in data_chunk.columns:\n",
    "                    tokenized = tokenizer.texts_to_sequences(data_chunk[feature].fillna(\"\").astype(str))\n",
    "                    padded = pad_sequences(tokenized, maxlen=10, padding=\"post\", truncating=\"post\")\n",
    "                    static_inputs.append(padded)\n",
    "\n",
    "            # Combine static inputs into a single array\n",
    "            static_inputs_combined = np.concatenate(\n",
    "                [np.expand_dims(arr, axis=1) if arr.ndim == 1 else arr for arr in static_inputs],\n",
    "                axis=1\n",
    "            ).astype(\"float32\")  # Ensure memory-efficient dtype\n",
    "\n",
    "            # Preprocess temporal inputs (numerical features)\n",
    "            temporal_inputs = []\n",
    "            for feature in numerical_features:\n",
    "                if feature in data_chunk.columns:\n",
    "                    temporal_inputs.append(data_chunk[feature].fillna(0).astype(\"float32\").values.reshape(-1, 1))\n",
    "            temporal_inputs_combined = np.expand_dims(\n",
    "                np.concatenate(temporal_inputs, axis=1), axis=1\n",
    "            )  # Add sequence dimension\n",
    "\n",
    "            \n",
    "\n",
    "            # Extract and process labels\n",
    "            if \"Label\" in data_chunk.columns:\n",
    "                labels = data_chunk[\"Label\"].values\n",
    "                labels = np.where(labels == 2, 1, labels).astype(\"int32\")  # Ensure int32 dtype\n",
    "            else:\n",
    "                raise KeyError(\"The 'Label' column is missing from the dataset.\")\n",
    "\n",
    "            # Yield combined inputs and labels\n",
    "            yield {\"static_input\": static_inputs_combined, \"temporal_input\": temporal_inputs_combined}, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\"  \n",
    "chunk_size = 10000  # Number of rows to read per chunk\n",
    "\n",
    "# Define features\n",
    "hash_features = [\n",
    "    \"Level_hash\", \"Task_hash\", \"Protocol_hash\", \"Version2_hash\", \"SchemaVersion_hash\",\n",
    "    \"Signature_hash\", \"EventType_hash\", \"StartFunction_hash\", \"ID_hash\",\n",
    "    \"Configuration_hash\", \"ConfigurationFileHash_hash\", \"IntegrityLevel_hash\",\n",
    "    \"UserID_hash\", \"Computer_hash\", \"RuleName_hash\", \"TerminalSessionId_hash\",\n",
    "    \"Version_hash\", \"User_hash\", \"ParentUser_hash\", \"FileVersion_hash\",\n",
    "    \"Guid_hash\", \"ProcessGuid_hash\", \"ParentProcessGuid_hash\", \"SourceProcessGUID_hash\",\n",
    "    \"TargetProcessGUID_hash\", \"SourceProcessGuid_hash\", \"LogonGuid_hash\",\n",
    "    \"ThreadID_hash\", \"LogonId_hash\", \"SourceHostname_hash\", \"OriginalFileName_hash\",\n",
    "    \"TargetProcessGuid_hash\", \"DestinationHostname_hash\", \"SourcePortName_hash\",\n",
    "    \"DestinationPortName_hash\", \"StartAddress_hash\", \"StartModule_hash\", \"NewThreadId_hash\",\n",
    "    \"GrantedAccess_hash\", \"QueryName_hash\", \"QueryResults_hash\", \"Hashes_hash\", \n",
    "    \"Hash_hash\", \"Contents_hash\", \"SourceIp_hash\", \"SourceIp_prefix_hash\", \n",
    "    \"DestinationIp_hash\", \"DestinationIp_prefix_hash\", \"EventID_hash\", \n",
    "    \"ProcessID_hash\", \"Execution_ProcessID_hash\", \"ParentProcessId_hash\", \n",
    "    \"TargetProcessId_hash\", \"EventRecordID_hash\", \"SourcePort_hash\", \n",
    "    \"DestinationPort_hash\", \"ProcessId_hash\"\n",
    "]\n",
    " # Replace with your hash features\n",
    "path_features = [\n",
    "    \"Image\", \"ParentImage\", \"SourceImage\", \"TargetImage\", \"ImageLoaded\",\n",
    "    \"CurrentDirectory\", \"ParentCommandLine\", \"CommandLine\", \"CallTrace\", \n",
    "    \"TargetFilename\", \"TargetObject\", \"Details\", \"PipeName\", \"QueryName\"\n",
    "]\n",
    "\n",
    "  # Replace with your path features\n",
    "textual_features = [\n",
    "    \"Description\", \"Product\", \"Company\"\n",
    "]\n",
    " # Replace with your textual features\n",
    "numerical_features = [\n",
    "    \"SystemTime\", \"UtcTime\", \"CreationUtcTime\", \"PreviousCreationUtcTime\"\n",
    "]\n",
    "  # Replace with your numerical features\n",
    "\n",
    "# Initialize tokenizer (use the one created earlier)\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts([])  # Adjust to fit your corpus if required\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_model_dir):\n",
    "    os.makedirs(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=epoch_model_path,  # Save model at the end of each epoch\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=best_model_path,  # Save the best model\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        monitor=\"val_loss\",  # Monitor validation loss\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Stop training if validation loss doesn't improve\n",
    "        patience=5,  # Number of epochs to wait\n",
    "        verbose=1,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training generator\n",
    "train_generator = data_generator_from_folder(\n",
    "    folder_path=\"F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\",\n",
    "    chunk_size=10000,\n",
    "    hash_features=hash_features,\n",
    "    path_features=path_features,\n",
    "    textual_features=textual_features,\n",
    "    numerical_features=numerical_features,\n",
    "    tokenizer=tokenizer,\n",
    "    vocab_size=15000,\n",
    "    split_ratio=0.8,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "# Create validation generator\n",
    "validation_generator = data_generator_from_folder(\n",
    "    folder_path=\"F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\",\n",
    "    chunk_size=10000,\n",
    "    hash_features=hash_features,\n",
    "    path_features=path_features,\n",
    "    textual_features=textual_features,\n",
    "    numerical_features=numerical_features,\n",
    "    tokenizer=tokenizer,\n",
    "    vocab_size=15000,\n",
    "    split_ratio=0.8,\n",
    "    mode=\"validation\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "epochs = 20  # Number of training epochs\n",
    "steps_per_epoch = 100  # Number of steps per epoch (based on chunk size and dataset size)\n",
    "validation_steps = 20  # Number of validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "tft_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",  # For binary classification\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2022 [870K Elements][Labelled].csv\n",
      "Static input shape: (8000, 217)\n",
      "Temporal input shape: (8000, 1, 4)\n",
      "Labels shape: (8000,)\n"
     ]
    }
   ],
   "source": [
    "# Test the generator\n",
    "gen = data_generator_from_folder(\n",
    "    folder_path=\"F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\",\n",
    "    chunk_size=10000,\n",
    "    hash_features=hash_features,\n",
    "    path_features=path_features,\n",
    "    textual_features=textual_features,\n",
    "    numerical_features=numerical_features,\n",
    "    tokenizer=tokenizer,\n",
    "    vocab_size=15000,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "inputs, labels = next(gen)\n",
    "print(\"Static input shape:\", inputs[\"static_input\"].shape)\n",
    "print(\"Temporal input shape:\", inputs[\"temporal_input\"].shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2022 [870K Elements][Labelled].csv\n",
      "Epoch 1/30\n",
      "\u001b[1m 86/532\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:54\u001b[0m 661ms/step - accuracy: 0.8690 - loss: 2301.5073Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [1.75M Elements][Labelled]checked.csv\n",
      "\u001b[1m261/532\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3:37\u001b[0m 804ms/step - accuracy: 0.9441 - loss: 952.0029Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [1.87M Elements][Labelled]checked.csv\n",
      "\u001b[1m452/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:07\u001b[0m 844ms/step - accuracy: 0.9533 - loss: 1204.2612Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [2.3M Elements][Labelled]checked.csv\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857ms/step - accuracy: 0.9524 - loss: 1580.7924Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2022 [870K Elements][Labelled].csv\n",
      "Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [1.75M Elements][Labelled]checked.csv\n",
      "\n",
      "Epoch 1: saving model to F:/models/model_epoch_01.keras\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 356.65900, saving model to F:/models/best_model.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 909ms/step - accuracy: 0.9524 - loss: 1584.5232 - val_accuracy: 0.9895 - val_loss: 356.6590\n",
      "Epoch 2/30\n",
      "\u001b[1m137/532\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:51\u001b[0m 890ms/step - accuracy: 0.9657 - loss: 519.0962Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [1.87M Elements][Labelled]checked.csv\n",
      "\n",
      "Epoch 2: saving model to F:/models/model_epoch_02.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 356.65900\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 285ms/step - accuracy: 0.9630 - loss: 472.2689 - val_accuracy: 0.1060 - val_loss: 49407.8984\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: saving model to F:/models/model_epoch_03.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 356.65900\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 55ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 50698.4883\n",
      "Epoch 4/30\n",
      "Processing file: F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\\LMD-2023 [2.3M Elements][Labelled]checked.csv\n",
      "\n",
      "Epoch 4: saving model to F:/models/model_epoch_04.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 356.65900\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 64ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4111 - val_loss: 29709.1562\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: saving model to F:/models/model_epoch_05.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 356.65900\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1916 - val_loss: 41320.0859\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: saving model to F:/models/model_epoch_06.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: saving model to F:/models/model_epoch_07.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: saving model to F:/models/model_epoch_08.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: saving model to F:/models/model_epoch_09.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: saving model to F:/models/model_epoch_10.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: saving model to F:/models/model_epoch_11.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: saving model to F:/models/model_epoch_12.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: saving model to F:/models/model_epoch_13.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: saving model to F:/models/model_epoch_14.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: saving model to F:/models/model_epoch_15.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: saving model to F:/models/model_epoch_16.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: saving model to F:/models/model_epoch_17.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: saving model to F:/models/model_epoch_18.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: saving model to F:/models/model_epoch_19.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: saving model to F:/models/model_epoch_20.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: saving model to F:/models/model_epoch_21.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: saving model to F:/models/model_epoch_22.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: saving model to F:/models/model_epoch_23.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: saving model to F:/models/model_epoch_24.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: saving model to F:/models/model_epoch_25.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: saving model to F:/models/model_epoch_26.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: saving model to F:/models/model_epoch_27.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: saving model to F:/models/model_epoch_28.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: saving model to F:/models/model_epoch_29.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: saving model to F:/models/model_epoch_30.keras\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18b2a73f0d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=532,  # Adjust based on your data size\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=134,  # Adjust based on your data size\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_steps(folder_path, chunk_size, split_ratio=0.8):\n",
    "#     \"\"\"\n",
    "#     Calculate total steps for training and validation based on the number of rows in the dataset.\n",
    "\n",
    "#     Args:\n",
    "#         folder_path (str): Path to the folder containing CSV files.\n",
    "#         chunk_size (int): Number of rows per chunk.\n",
    "#         split_ratio (float): Proportion of data for training (e.g., 0.8 for 80%).\n",
    "\n",
    "#     Returns:\n",
    "#         (int, int): Steps for training and validation.\n",
    "#     \"\"\"\n",
    "#     total_rows = 0\n",
    "#     csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "#     for file_path in csv_files:\n",
    "#         # Count rows in each file\n",
    "#         total_rows += sum(1 for _ in open(file_path)) - 1  # Subtract 1 for the header\n",
    "\n",
    "#     # Calculate steps\n",
    "#     total_chunks = total_rows // chunk_size\n",
    "#     train_steps = int(total_chunks * split_ratio)\n",
    "#     val_steps = total_chunks - train_steps\n",
    "\n",
    "#     return train_steps, val_steps\n",
    "\n",
    "# folder_path = \"F:/Intrusion detection datasets/Lateral-Movement-Dataset--LMD_Collections/LMD_essential_processed\"\n",
    "# chunk_size = 10000\n",
    "\n",
    "# # Calculate steps dynamically\n",
    "# steps_per_epoch, validation_steps = calculate_steps(folder_path, chunk_size, split_ratio=0.8)\n",
    "\n",
    "# print(f\"Steps per epoch (training): {steps_per_epoch}\")\n",
    "# print(f\"Validation steps: {validation_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
